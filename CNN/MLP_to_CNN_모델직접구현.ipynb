{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "x6zUg48qUGtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Activation Function"
      ],
      "metadata": {
        "id": "EdkrtsXD-E9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMPDOo9h6ejt"
      },
      "outputs": [],
      "source": [
        "# Acivation Function\n",
        "t\n",
        "class ReLU(Layer):    \n",
        "    def __init__(self, output_dim):\n",
        "        self.units = output_dim\n",
        "        self.type = 'ReLU'\n",
        " \n",
        "    def __str__(self):\n",
        "        return f\"{self.type} Layer\"       \n",
        "         \n",
        "    def forward(self, input_val):\n",
        "        self._prev_acti = np.maximum(0, input_val)\n",
        "        return self._prev_acti\n",
        "     \n",
        "    def backward(self, dJ):\n",
        "        return dJ * np.heaviside(self._prev_acti, 0)\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        self.units = output_dim\n",
        "        self.type = 'Sigmoid'\n",
        " \n",
        "    def __str__(self):\n",
        "        return f\"{self.type} Layer\"       \n",
        "         \n",
        "    def forward(self, input_val):\n",
        "        self._prev_acti = 1 / (1 + np.exp(-input_val))\n",
        "        return self._prev_acti\n",
        "     \n",
        "    def backward(self, dJ):\n",
        "        sig = self._prev_acti\n",
        "        return dJ * sig * (1 - sig)\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        self.units = output_dim\n",
        "        self.type = 'Softmax'\n",
        "\n",
        "    def softmax(a):\n",
        "      C = np.max(a)\n",
        "      exp_a = np.exp(a - C)\n",
        "      if a.ndim == 1:\n",
        "          sum_exp_a = np.sum(exp_a)\n",
        "          y = exp_a / sum_exp_a\n",
        "      else:\n",
        "          sum_exp_a = np.sum(exp_a, 1)\n",
        "          sum_exp_a = sum_exp_a.reshape(sum_exp_a.shape[0], 1)\n",
        "          y = exp_a / sum_exp_a\n",
        "      return y\n",
        "\n",
        "    def cross_entropy_loss(y, t):\n",
        "    C = 1e-7\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(t * np.log(y + C)) / batch_size\n",
        "    \n",
        "    def forward(self, input_val, t):\n",
        "        self.t = t\n",
        "        self._prev_acti = softmax(input_val)\n",
        "        self.loss = cross_entropy_loss(self.y, self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "        return dx\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.weights = np.random.rand(output_dim, input_dim)\n",
        "        self.biases = np.random.rand(output_dim, 1)\n",
        "        self.type = 'Linear'\n",
        " \n",
        "    def __str__(self):\n",
        "        return f\"{self.type} Layer\"\n",
        "         \n",
        "    def forward(self, input_val):\n",
        "        self._prev_acti = input_val\n",
        "        return np.matmul(self.weights, input_val) + self.biases\n",
        "     \n",
        "    def backward(self, dA):\n",
        "        dW = np.dot(dA, self._prev_acti.T)\n",
        "        dB = dA.mean(axis=1, keepdims=True)\n",
        "         \n",
        "        delta = np.dot(self.weights.T, dA)\n",
        "         \n",
        "        return delta, dW, dB\n",
        "     \n",
        "    def optimize(self, dW, dB, rate):\n",
        "        self.weights = self.weights - rate * dW\n",
        "        self.biases = self.biases - rate * dB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function"
      ],
      "metadata": {
        "id": "uYYJD-zH-LEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function\n",
        "class MeanSquaredError(Layer):\n",
        "    def __init__(self, predicted, real):\n",
        "        self.predicted = predicted\n",
        "        self.real = real\n",
        "        self.type = 'Mean Squared Error'\n",
        "     \n",
        "    def forward(self):\n",
        "        return np.power(self.predicted - self.real, 2).mean()\n",
        " \n",
        "    def backward(self):\n",
        "        return 2 * (self.predicted - self.real).mean()\n",
        "\n",
        "class BinaryCrossEntropy(Layer):\n",
        "    def __init__(self, predicted, real):\n",
        "        self.real = real\n",
        "        self.predicted = predicted\n",
        "        self.type = 'Binary Cross-Entropy'\n",
        "     \n",
        "    def forward(self):\n",
        "        n = len(self.real)\n",
        "        loss = np.nansum(-self.real * np.log(self.predicted) - (1 - self.real) * np.log(1 - self.predicted)) / n\n",
        "         \n",
        "        return np.squeeze(loss)\n",
        "     \n",
        "    def backward(self):\n",
        "        n = len(self.real)\n",
        "        return (-(self.real / self.predicted) + ((1 - self.real) / (1 - self.predicted))) / n\n",
        "\n"
      ],
      "metadata": {
        "id": "aHFqwM4b697r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Layer Perceptron Model (Sigmoid)"
      ],
      "metadata": {
        "id": "QMGP-XvI-OMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Function (Sigmoid)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def Backpropagation(f, input_data): #numerical_derivative 수치 미분 함수\n",
        "    delta_x = 1e-4\n",
        "\n",
        "    ret = np.zeros_like(input_data)\n",
        "    it = np.nditer(input_data, flags=['multi_index'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "\n",
        "        tmp = input_data[idx]\n",
        "        input_data[idx] = float(tmp) + delta_x\n",
        "        fx1 = f(input_data)\n",
        "\n",
        "        input_data[idx] = float(tmp) - delta_x\n",
        "        fx2 = f(input_data)\n",
        "\n",
        "        ret[idx] = (fx1 - fx2) / (2 * delta_x)\n",
        "        input_data[idx] = tmp\n",
        "        it.iternext()\n",
        "\n",
        "    return ret\n",
        "\n",
        "class MLPmodel:\n",
        "  def __init__(self, model_name, x, t):\n",
        "    self.name = model_name\n",
        "\n",
        "    #input data\n",
        "    self.__x = x\n",
        "    self.__t = t\n",
        "\n",
        "    #weight, bias\n",
        "    i =  [[ 2.63259021,  2.44820463 , 4.63134428 , 1.17225176 , 5.48438174,  7.2549209 ],[ 3.28485616 ,-1.69787055,  5.22847983 , 2.02503399 , 5.40761403  ,6.48606646]]\n",
        "    self.__W = np.array(i)\n",
        "    j = [-4.61606264  ,1.46435451, -7.54664825 ,-2.47371219 ,-2.32749526, -3.09309171]\n",
        "    self.__b = np.array(j)\n",
        "    k = [[ -6.64307767],[ -4.60866687],[-11.09625077],[ -4.26531046],[  8.01868687],[ 11.17017424]]\n",
        "    self.__W2 = np.array(k)\n",
        "    g = [-4.44898273]\n",
        "    self.__b2 = np.array(g)\n",
        "\n",
        "    #learning rate, loss function\n",
        "    self.__learning_rate = 1e-1\n",
        "    self.loss_func = self.__feed_forward\n",
        "\n",
        "\n",
        "  def __feed_forward(self):\n",
        "    delta = 1e-7\n",
        "\n",
        "    #input -> hidden\n",
        "    y = np.dot(self.__x, self.__W) + self.__b\n",
        "    y_hat = sigmoid(y) # Activation Function\n",
        "    #hidden -> output\n",
        "    y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "    y2_hat = sigmoid(y2)\n",
        "\n",
        "    #CrossEntropy로 Error detection\n",
        "    return -np.sum(self.__t * np.log(y2_hat + delta) + (1 - self.__t) * np.log(1 - y2_hat + delta))\n",
        "\n",
        "  \n",
        "\n",
        "  def train(self):\n",
        "    f = lambda x: self.__feed_forward()\n",
        "\n",
        "    for step in range(8001):\n",
        "            self.__W -= self.__learning_rate * Backpropagation(f, self.__W)\n",
        "            self.__b -= self.__learning_rate * Backpropagation(f, self.__b)\n",
        "            self.__W2 -= self.__learning_rate * Backpropagation(f, self.__W2)\n",
        "            self.__b2 -= self.__learning_rate * Backpropagation(f, self.__b2)\n",
        "\n",
        "  def predict(self, x):\n",
        "    #input -> hidden\n",
        "    y = np.dot(x, self.__W) + self.__b\n",
        "    y_hat = sigmoid(y) # Activation Function\n",
        "    #hidden -> output\n",
        "    y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "    y2_hat = sigmoid(y2)\n",
        "\n",
        "    if y2_hat < 0.5:\n",
        "            return 0, y2_hat\n",
        "    return 1, y2_hat\n",
        "\n",
        "    #if y2_hat <0.5 -> 0출력, 아니면 1출력 -> sigmoid 출력"
      ],
      "metadata": {
        "id": "YIwHqBV--SIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data input\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape([4,2])\n",
        "y = np.array([0,1,1,0]).reshape([4,1])\n",
        "\n",
        "#train\n",
        "MLP = MLPmodel(\"MLP\", x, y)\n",
        "MLP.train()\n",
        "\n",
        "#output\n",
        "print(MLP.predict([0,0]))\n",
        "print(MLP.predict([1,0]))\n",
        "print(MLP.predict([0,1]))\n",
        "print(MLP.predict([1,1]))"
      ],
      "metadata": {
        "id": "2rAg9HNEWDdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Layer Perceptron (Softmax)"
      ],
      "metadata": {
        "id": "CbH8KTR-fypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "M8fyLcVbpuGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    c = np.max(x)\n",
        "    minus = x-c\n",
        "    exp_b = np.exp(minus)\n",
        "    sum_exp_b = np.sum(exp_b)\n",
        "    return exp_b /sum_exp_b\n",
        "\n",
        "\n",
        "k = [0.3,2,0.3]\n",
        "\n",
        "print(softmax(k))"
      ],
      "metadata": {
        "id": "8Mloh6hlsi2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y3_hat = [0.3,0.6,0.4]\n",
        "k = np.max(y3_hat, axis=0)\n",
        "i = 0\n",
        "for i in range(2):\n",
        "  if y3_hat[i] == k:\n",
        "    y3_hat[i] = 1\n",
        "print(y3_hat)\n"
      ],
      "metadata": {
        "id": "baGSD1rr1TYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y3_hat = [0.3,0.6,0.4]\n",
        "\n",
        "k = np.max(y3_hat, axis=0)\n",
        "i = 0\n",
        "for i in range(2):\n",
        "  if y3_hat[i] == k:\n",
        "        y3_hat[i] = 1\n",
        "if(y3_hat[0]==1):\n",
        "  print(1)\n",
        "if(y3_hat[1]==1):\n",
        "  print(2)\n",
        "if(y3_hat[2]==1):\n",
        "  print(3)"
      ],
      "metadata": {
        "id": "WfCBWKuG3QBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=15)\n",
        "def softmax(x):\n",
        "    exp_a = np.exp(x)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    return y\n",
        "\n",
        "y3 = [[67.34759010386061 ,60.49088997410166 ,61.34620329299571]]\n",
        "y3_hat = softmax(y3)\n",
        "print(y3_hat)"
      ],
      "metadata": {
        "id": "ez_5U8y7-gkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Function (Sigmoid)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    c = np.max(x)\n",
        "    minus = x-c\n",
        "    exp_b = np.exp(minus)\n",
        "    sum_exp_b = np.sum(exp_b)\n",
        "    return exp_b /sum_exp_b\n",
        "\n",
        "def relu(x):\n",
        "    return(np.maximum(0,x))\n",
        "\n",
        "def Leaky_ReLU(x):\n",
        "  return (np.maximum(0.01*x,x))\n",
        "\n",
        "def Backpropagation(f, input_data): #numerical_derivative 수치 미분 함수\n",
        "    delta_x = 1e-4\n",
        "\n",
        "    ret = np.zeros_like(input_data)\n",
        "    it = np.nditer(input_data, flags=['multi_index'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "\n",
        "        tmp = input_data[idx]\n",
        "        input_data[idx] = float(tmp) + delta_x\n",
        "        fx1 = f(input_data)\n",
        "\n",
        "        input_data[idx] = float(tmp) - delta_x\n",
        "        fx2 = f(input_data)\n",
        "\n",
        "        ret[idx] = (fx1 - fx2) / (2 * delta_x)\n",
        "        input_data[idx] = tmp\n",
        "        it.iternext()\n",
        "\n",
        "    return ret\n",
        "\n",
        "class MLPmodel:\n",
        "  \n",
        "  def __init__(self, model_name, x_data, t):\n",
        "    self.name = model_name\n",
        "\n",
        "    #input data\n",
        "    self.__x = x_data\n",
        "    self.__t = t\n",
        "\n",
        "    #weight, bias\n",
        "    i =  [[ 2.63259021,  2.44820463 , 4.63134428 , 1.17225176 , 5.48438174,  7.2549209 ],[ 3.28485616 ,-1.69787055,  5.22847983 , 2.02503399 , 5.40761403  ,6.48606646]]\n",
        "    self.__W = np.random.rand(4,12)\n",
        "    j = [-4.61606264  ,1.46435451, -7.54664825 ,-2.47371219 ,-2.32749526, -3.09309171]\n",
        "    self.__b = np.random.rand(1,12)\n",
        "    k = [[ -6.64307767],[ -4.60866687],[-11.09625077],[ -4.26531046],[  8.01868687],[ 11.17017424]]\n",
        "    self.__W2 = np.random.rand(12,12)\n",
        "    g = [-4.44898273]\n",
        "    self.__b2 = np.random.rand(1,12)\n",
        "    self.__W3 = np.random.rand(12,3)\n",
        "    self.__b3 = np.random.rand(1,3)\n",
        "\n",
        "    #learning rate, loss function\n",
        "    self.__learning_rate = 0.01\n",
        "    self.loss_func = self.__feed_forward\n",
        "\n",
        "  \n",
        "  def __feed_forward(self):\n",
        "    delta = 1e-7\n",
        "\n",
        "    #input -> hidden\n",
        "    y = np.dot(self.__x, self.__W) + self.__b\n",
        "    y_hat = Leaky_ReLU(y) # Activation Function\n",
        "    #hidden -> hidden2\n",
        "    y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "    y2_hat = Leaky_ReLU(y2)\n",
        "    #hidden2 -> output\n",
        "    y3 = np.dot(y2_hat, self.__W3) + self.__b3\n",
        "    y3_hat = softmax(y3)\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    loss_cce  = cce(self.__t,   y3_hat)\n",
        "    return loss_cce\n",
        "\n",
        "  \n",
        "\n",
        "  def train(self):\n",
        "    f = lambda x: self.__feed_forward()\n",
        "\n",
        "    for step in range(8001):\n",
        "            self.__W -= self.__learning_rate * Backpropagation(f, self.__W)\n",
        "            self.__b -= self.__learning_rate * Backpropagation(f, self.__b)\n",
        "            self.__W2 -= self.__learning_rate * Backpropagation(f, self.__W2)\n",
        "            self.__b2 -= self.__learning_rate * Backpropagation(f, self.__b2)\n",
        "            self.__W3 -= self.__learning_rate * Backpropagation(f, self.__W3)\n",
        "            self.__b3 -= self.__learning_rate * Backpropagation(f, self.__b3)\n",
        "\n",
        "  def predict(self, x_data):\n",
        "    #input -> hidden\n",
        "    print(x_data)\n",
        "    y = np.dot(x_data, self.__W) + self.__b\n",
        "    print(self.__W)\n",
        "    y_hat = Leaky_ReLU(y) # Activation Function\n",
        "    print(y_hat)\n",
        "    \n",
        "    #hidden -> hidden2\n",
        "    y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "    y2_hat = Leaky_ReLU(y2)\n",
        "    print(y2_hat)\n",
        "    #hidden2 -> output\n",
        "    y3 = np.dot(y2_hat, self.__W3) + self.__b3\n",
        "    y3_hat = softmax(y3)\n",
        "    print(y3_hat)\n",
        "    for i in range(0,3):\n",
        "      if y3_hat[0][i]==max(y3_hat[0]):\n",
        "        return i+1\n",
        "\n",
        "\n",
        "    #if y2_hat <0.5 -> 0출력, 아니면 1출력 -> sigmoid 출력"
      ],
      "metadata": {
        "id": "pDUqlFwsf2DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output one-hot-encoding\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/kaggle/y.csv')\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "data_2 = data.apply(le.fit_transform)\n",
        "\n",
        "enc = preprocessing.OneHotEncoder()\n",
        "\n",
        "enc.fit(data_2)\n",
        "\n",
        "onehotlabels = enc.transform(data_2).toarray()\n",
        "\n",
        "#data input\n",
        "from numpy import genfromtxt\n",
        "x = genfromtxt('/content/gdrive/MyDrive/kaggle/x.csv', delimiter=',', skip_header = 1)\n",
        "y = onehotlabels\n",
        "#train\n",
        "MLP = MLPmodel(\"MLP\", x, y)\n",
        "MLP.train()"
      ],
      "metadata": {
        "id": "wDlYnD7wkpKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(MLP.predict([5.1],[3.5],[1.4],[0.2]]))\n",
        "for i in range(149):\n",
        "  print(MLP.predict(x[i]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KPbXRTbG7UjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import genfromtxt\n",
        "data = genfromtxt('/content/gdrive/MyDrive/kaggle/y.csv', delimiter=',', skip_header = 1)\n",
        "print(onehotlabels)"
      ],
      "metadata": {
        "id": "0b5yq_FNlnXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/kaggle/y.csv')\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "data_2 = data.apply(le.fit_transform)\n",
        "\n",
        "enc = preprocessing.OneHotEncoder()\n",
        "\n",
        "enc.fit(data_2)\n",
        "\n",
        "onehotlabels = enc.transform(data_2).toarray()\n",
        "onehotlabels\n"
      ],
      "metadata": {
        "id": "EougBfaKnPjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP 2"
      ],
      "metadata": {
        "id": "tC5qATlZNY_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
        "        assert(len(layers) == len(activations)+1)\n",
        "        self.layers = layers\n",
        "        self.activations = activations\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layers)-1):\n",
        "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        # return the feedforward value for x\n",
        "        a = np.copy(x)\n",
        "        z_s = []\n",
        "        a_s = [a]\n",
        "        for i in range(len(self.weights)):\n",
        "            activation_function = self.getActivationFunction(self.activations[i])\n",
        "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
        "            a = activation_function(z_s[-1])\n",
        "            a_s.append(a)\n",
        "        return (z_s, a_s)\n",
        "    def backpropagation(self,y, z_s, a_s):\n",
        "        dw = []  # dC/dW\n",
        "        db = []  # dC/dB\n",
        "        deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
        "        # insert the last layer error\n",
        "        deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
        "        # Perform BackPropagation\n",
        "        for i in reversed(range(len(deltas)-1)):\n",
        "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
        "        #a= [print(d.shape) for d in deltas]\n",
        "        batch_size = y.shape[1]\n",
        "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
        "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
        "        # return the derivitives respect to weight matrix and biases\n",
        "        return dw, db\n",
        "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
        "# update weights and biases based on the output\n",
        "        for e in range(epochs): \n",
        "            print(\"epoch :\",e+1,'\\n')\n",
        "            i=0\n",
        "            while(i<len(y)):\n",
        "                x_batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                i = i+batch_size\n",
        "                z_s, a_s = self.feedforward(x_batch)\n",
        "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
        "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
        "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
        "                print(\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch) ))\n",
        "    @staticmethod\n",
        "    def getActivationFunction(name):\n",
        "        if(name == 'sigmoid'):\n",
        "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
        "        elif(name == 'linear'):\n",
        "            return lambda x : x\n",
        "        elif(name == 'relu'):\n",
        "            def relu(x):\n",
        "                y = np.copy(x)\n",
        "                y[y<0] = 0\n",
        "                return y\n",
        "            return relu\n",
        "        else:\n",
        "            print('Unknown activation function. linear is used')\n",
        "            return lambda x: x\n",
        "    \n",
        "    @staticmethod\n",
        "    def getDerivitiveActivationFunction(name):\n",
        "        if(name == 'sigmoid'):\n",
        "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
        "            return lambda x :sig(x)*(1-sig(x)) \n",
        "        elif(name == 'linear'):\n",
        "            return lambda x: 1\n",
        "        elif(name == 'relu'):\n",
        "            def relu_diff(x):\n",
        "                y = np.copy(x)\n",
        "                y[y>=0] = 1\n",
        "                y[y<0] = 0\n",
        "                return y\n",
        "            return relu_diff\n",
        "        else:\n",
        "            print('Unknown activation function. linear is used')\n",
        "            return lambda x: 1"
      ],
      "metadata": {
        "id": "K3HSg26-NYnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    import matplotlib.pyplot as plt\n",
        "    nn = NeuralNetwork([1, 100, 1],activations=['sigmoid', 'sigmoid'])\n",
        "    X = 2*np.pi*np.random.rand(3000).reshape(1, -1)\n",
        "    y = np.sin(X)\n",
        "    \n",
        "    nn.train(X, y, epochs=1000, batch_size=64, lr = .1)\n",
        "    _, a_s = nn.feedforward(X)\n",
        "    #print(y, X)\n",
        "    plt.scatter(X.flatten(), y.flatten())\n",
        "    plt.scatter(X.flatten(), a_s[-1].flatten())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dVngCBIvNb9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN model"
      ],
      "metadata": {
        "id": "NgRtuNBWKnKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CNNmodel:\n",
        "\n",
        "  def __init__(self, x, hidden, y):\n",
        "        self.__x = x\n",
        "        self.__hidden = hidden\n",
        "        self.__y = y\n",
        "\n",
        "        self.__learning_rate = 1e-3\n",
        "\n",
        "        self.__W = np.random.rand(x, hidden)\n",
        "        self.__b = np.random.rand(hidden)\n",
        "\n",
        "        self.__W2 = np.random.rand(hidden, y)\n",
        "        self.__b2 = np.random.rand(y)\n",
        "\n",
        "        self.__data = np.array([])\n",
        "        self.__one_hot = np.array([])\n",
        "\n",
        "  # Activation Function (Sigmoid)\n",
        "  def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def Backpropagation(f, input_data): #numerical_derivative 수치 미분 함수\n",
        "    delta_x = 1e-4\n",
        "\n",
        "    ret = np.zeros_like(input_data)\n",
        "    it = np.nditer(input_data, flags=['multi_index'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "\n",
        "        tmp = input_data[idx]\n",
        "        input_data[idx] = float(tmp) + delta_x\n",
        "        fx1 = f(input_data)\n",
        "\n",
        "        input_data[idx] = float(tmp) - delta_x\n",
        "        fx2 = f(input_data)\n",
        "\n",
        "        ret[idx] = (fx1 - fx2) / (2 * delta_x)\n",
        "        input_data[idx] = tmp\n",
        "        it.iternext()\n",
        "\n",
        "    return ret\n",
        "\n",
        "  # 순전파\n",
        "  def __feed_forward(self):\n",
        "        delta = 1e-7\n",
        "\n",
        "        y = np.dot(self.__x, self.__W) + self.__b\n",
        "        y_hat = sigmoid(y)\n",
        "\n",
        "        y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "        y2_hat = sigmoid(y2)\n",
        "\n",
        "        return -np.sum(self.__one_hot * np.log(y2_hat + delta) + (1 - self.__one_hot) * np.log(1 - y2_hat + delta))\n",
        "\n",
        "  def loss_val(self):\n",
        "        delta = 1e-7\n",
        "\n",
        "        y = np.dot(self.__x, self.__W) + self.__b\n",
        "        y_hat = sigmoid(y)\n",
        "\n",
        "        y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "        y2_hat = sigmoid(y2)\n",
        "\n",
        "        return -np.sum(self.__one_hot * np.log(y2_hat + delta) + (1 - self.__one_hot) * np.log(1 - y2_hat + delta))\n",
        "\n",
        "  # 예측값\n",
        "  def predict(self, x):\n",
        "    #input -> hidden\n",
        "    y = np.dot(self.__x, self.__W) + self.__b\n",
        "    y_hat = sigmoid(y) # Activation Function\n",
        "    #hidden -> output\n",
        "    y2 = np.dot(y_hat, self.__W2) + self.__b2\n",
        "    y2_hat = sigmoid(y2)\n",
        "    return np.argmax(y2_hat)\n",
        "\n",
        "  # 정확도\n",
        "  def accuracy(self, test_data):\n",
        "        count = 0\n",
        "        for index in range(len(test_data)):\n",
        "            label = int(test_data[index, 0])\n",
        "            data = test_data[index, 1:]\n",
        "            predicted_val = self.predict(data)\n",
        "            if predicted_val == label:\n",
        "                count += 1\n",
        "\n",
        "        print('Accuracy = ', 100 * (count / float(len(test_data))))\n",
        "\n",
        "  def train(self, train_data):\n",
        "        label = int(train_data[0])\n",
        "        self.__data = (train_data[1:] / 255.0 * 0.99) + 0.01\n",
        "        self.__one_hot = np.zeros(self.__y) + 0.01\n",
        "        self.__one_hot[label] = 0.99\n",
        "\n",
        "        f = lambda x: self.__feed_forward()\n",
        "        self.__W -= self.__learning_rate * Backpropagation(f, self.__W)\n",
        "        self.__b -= self.__learning_rate * Backpropagation(f, self.__b)\n",
        "        self.__W2 -= self.__learning_rate * Backpropagation(f, self.__W2)\n",
        "        self.__b2 -= self.__learning_rate * Backpropagation(f, self.__b2)\n"
      ],
      "metadata": {
        "id": "F-5z9BMiKqRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/kaggle/Iris.csv')\n",
        "test_data = pd.read_csv('/content/gdrive/MyDrive/kaggle/test.txt')\n",
        "\n",
        "mnist = CNNmodel(784, 50, 10)\n",
        "for step in range(30001):\n",
        "    mnist.train(train_data[step])\n",
        "\n",
        "    if step % 400 == 0:\n",
        "        print('step = ', step, 'loss_val = ', mnist.loss_val())\n",
        "\n",
        "mnist.accuracy(test_data)"
      ],
      "metadata": {
        "id": "wyw2A7xwRg3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}