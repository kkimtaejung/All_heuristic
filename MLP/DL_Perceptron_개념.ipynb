{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mtOvdS4x2Y9c",
        "7rdJLX18xHpL",
        "2KfsOb0r6js2",
        "4OI-eXsN7Kyx",
        "6LRPvuKc9Y56",
        "u3a3Wy4u-RnW",
        "LFkUenEYxRzu",
        "Qk8IC6uZ1_EV",
        "AnHceHB39WbS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 출처"
      ],
      "metadata": {
        "id": "mtOvdS4x2Y9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/wikibook/machine-learning/tree/2.0/jupyter_notebook"
      ],
      "metadata": {
        "id": "jixBZqqUwkZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝 개념"
      ],
      "metadata": {
        "id": "7rdJLX18xHpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN 심층신경망\n",
        "\n",
        "X-NN 구조 ( 대표적으로 CNN,RNN )...\n",
        "\n",
        "뉴런 == 노드\n",
        "\n",
        "레이어 == 뉴런이 일렬로 존재하는 공간\n",
        "\n",
        "가중치 == 뉴런과 뉴런끼리 연결된 선위의 변수\n",
        "\n",
        "다음 뉴런이 받는 정보 == 뉴런이 가진 정보 * 가중치"
      ],
      "metadata": {
        "id": "3XtXn99ZxKTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝 명칭 유래"
      ],
      "metadata": {
        "id": "2KfsOb0r6js2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex) x -> f(x) -> g(f(x)) -> h(g(f(x)))\n",
        "\n",
        "레이어를 건너갈수록 계산 과정이 함수 안의 함수로 깊게 이어짐 => 딥러닝"
      ],
      "metadata": {
        "id": "EogDsu3l6vJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 퍼셉트론"
      ],
      "metadata": {
        "id": "4OI-eXsN7Kyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 뉴런 하나로 AND, OR 연산 해결  \n",
        "* ex) 두 입력 x1,x2에 대해 가중치 w1,w2와 곱해지고, 편향값(bias)가 더해져, z = w1*x1 + w2*x2 + bias 값을 통해 z를 구한다. z<0일경우 출력값은 a(z) = 0, z>=0일경우 출력값은 a(z) = 1  \n",
        "* 편향값은 효율적인 학습, 가중치 조정에 시간소요 덜어준다  \n",
        "* AND 연산 -> x1=1,x2=1일 경우 a(z) =1  \n",
        "ex) x1->0.6,x2->0.6,bias=-1 인경우  \n",
        "* OR도 마찬가지\n",
        "* AND, OR모두 하나의 의사결정선으로 값 분류 가능"
      ],
      "metadata": {
        "id": "w9qr34GP7NVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다층 퍼셉트론"
      ],
      "metadata": {
        "id": "6LRPvuKc9Y56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XOR 연산 가능\n",
        "* z1,z2... z의 수를 증가  \n",
        "* z1,z2 -> ztotal 추가로 하나의 선으로 표현가능\n",
        "\n",
        "* MLP (다층 퍼셉트론)\n",
        "1. 입력 레이어 x1,x2\n",
        "2. 히든 레이어 z1,z2\n",
        "3. 출력 레이어 ztotal\n"
      ],
      "metadata": {
        "id": "XM548trI9bR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 뉴런 (=노드)"
      ],
      "metadata": {
        "id": "u3a3Wy4u-RnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* x -> x*w -> 활성화 함수(비선형 함수)  \n",
        "* 스텝 함수 대신 비선형 함수를 활성화 함수로 사용하는 이유 -> 역전파 사용한 모델 학습 시 활성화 함수 미분 가능해야하기 때문  \n",
        "* 비선형 활성화 함수 :: sigmoid, TanH, ReLU..."
      ],
      "metadata": {
        "id": "4cGDEMeE-Upw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#딥러닝 학습\n"
      ],
      "metadata": {
        "id": "LFkUenEYxRzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##순전파\n",
        "* x -> f(x) -> g(x) -> y_hat (딥러닝의 출력값) / 정답 -> y 일때,\n",
        "순전파를 통해 y와 y_hat의 차이를 구할 수 있다.  \n",
        "* 손실함수 :: 출력값 & 정답 차이를 구하기 위한 함수  \n"
      ],
      "metadata": {
        "id": "_B37ayH4xYKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##손실함수\n",
        "* 정의 :: y, y_hat의 차이  \n",
        "* 특징 :: 차이가 일치할수록 값 작고, 차이가 불일치할수록 값 크다  \n",
        "* 매개변수(가중치,편향값) 조절 -> 손실함수 값 최저로 만드는 과정 -> 최적화 -> 옵티마이저 통해 이루어짐 -> 옵티마이저는 역전파 과정을 수행해서 최적화"
      ],
      "metadata": {
        "id": "5eFkX_eNxbZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##최적화\n",
        "* 방법 :: 경사하강법 (대표적)\n",
        "* 경사하강법 :: 솔심함수에 대한 모델 매개변수 미분값 구하고, 미분값 반대방향으로 매개변수 조절, 최저 손실함수 구함 -> 미분값의 변곡점  \n",
        " {매개변수 new = 매개변수 old - 학습률*dl/dw}\n",
        "\n",
        "* 글로벌 미니멈 :: 미분값의 변곡점중 가장 낮은 로컬 미니멈  \n",
        "* 실제론 여러 매개변수, 여러 레이어 -> 옵티마이저로 매개변수 조정 -> 역전파 사용\n"
      ],
      "metadata": {
        "id": "LotDDZgjx8Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##역전파 (가중치 조절)\n",
        "* 특징 :: 손실함수 값 최저로 함 = 모델 에러율 최저\n",
        "* ex) x->f(x)->g(x)->y_hat/ 가중치 w1,w2/ 편향값 b1,b2/ 손실함수 L 에 대해  \n",
        "[L에 대한 각 매개변수의 미분값 (연쇄법칙)]  \n",
        "dL/dw2 = dL/dg*dg/dw2  \n",
        "dL/db2 = dL/dg*dg/db2  \n",
        "dL/dw1 = dL/dg*dg/df*df/dw1  \n",
        "dL/db1 = dL/dg*dg/df*df/db1  \n",
        "[학습률 적용]  \n",
        "w2 new = w2 old - 학습률*dL/dw2  \n",
        "b2 new = b2 old - 학습률*dL/db2  \n",
        "w1 new = w1 old - 학습률*dL/dw1  \n",
        "b1 new = b1 old - 학습률*dL/db1  "
      ],
      "metadata": {
        "id": "DXT5kpMBzcPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##옵티마이저\n",
        "* 과정 :: \n",
        "1. 배치 경사하강법\n",
        "2. SGD\n",
        "3. 미니 배치\n",
        "4. 모멘텀\n",
        "5. 학습률\n",
        "6. Adagrad\n",
        "7. Adam\n",
        "***\n",
        "  \n",
        "1. \n",
        "  * 딥러닝 모델 최적화의 가장 기본적 방법\n",
        "  * 장 :: 1차 미분만으로 차이가 적은 딥러닝 모델\n",
        "  * 단 :: 매개변수 변경 시간 오래, 계산량 많, 로컬 미니멈이 많더라도 가장 가까운 로컬 미니멈에서 멈춤  \n",
        "2. \n",
        "  * 배치 경사하강법 -> 계산량, 시간 해결\n",
        "  * 하나의 데이터마다 매개변수 변경 = 매개변수가 껑충 변경\n",
        "  * 장 :: 제한된 자원으로도 딥러닝 학습가능, 로컬 미니멈이 많을때 유용\n",
        "  * 단 :: 최악의 결과가 나올수도 있음\n",
        "3. \n",
        "  * 1,2방법의 절충안\n",
        "  * 정해진 양만 계산해서 매개변수를 최적화\n",
        "  * 특징 :: 주기(주기만큼 데이터로 학습 진행), 배치 사이즈(배개변수 조정을 위한 N개의 데이터 사용, 배치 사이즈 N), 스텝(미니 배치로 매개변수 조정되는 순간)\n",
        "  -> 데이터 1000개, 배치사이즈 100, 10번 스텝 매개변수 조정, 10번 스텝 = 1주기\n",
        "  * 장 :: 학습 짧고, 경사하강법보다 손실 적은 지점까지 도달가능\n",
        "4. \n",
        "  * 근처 로컬 미니멈에서만 학습진행 -> 해결\n",
        "  * 공을 언덕에서 굴려서 너 낮은 로컬 미니멈까지 굴러가는 구조\n",
        "  -> 매개변수 new = 매개변수 old + 이동변수\n",
        "  -> 이동변수 = 모멘텀 - 학습률*dl/dw\n",
        "  -> 모멘텀 = 모멘텀조정률*이동변수\n",
        "  * 방법 :: 최초 모멘텀은 0-> 기울기 반대 방향(+)/모멘텀 방향(+) 이면 (+)방향으로 매개변수 조절-> 기울기가 0이어도 모멘텀에 의해 (+)로 매개변수 조절-> 기울기가 (-)면 (-)로 매개변수 조절 -> 더 나은 로컬 미니멈으로 도달\n",
        "  * 단 :: 글로벌 미니멈으로 간다는 보장은 없다\n",
        "5. \n",
        "  * 학습률 크면 -> 배개변수 변경치 커짐, 로컬 미니멈으로 수렴 안될수 있음\n",
        "  * 학습률 작으면 -> 학습 시간 오래걸림\n",
        "  * 초기에 학습률 크게하고, 어느정도 학습되면 학습률 작게 조절하여 효율적 학습가능\n",
        "  -> decay :: 학습률 조정 방식 (시간 기반, 스텝 기반...)\n",
        "6. \n",
        "  * decay의 한계 -> 각 가중치마다의 역할 고려 안하고 동일한 학습률 적용의 지나친 일반화, 처음에 로컬 미니멈과 멀다가 나중에 가까워진다는 보장 없음\n",
        "  * 개발자의 개입없이 직접 학습에 따라 가중치별로 학습률 지정 -> 변화가 많은 가중치는 학습률 적게/변화가 적은 가중치는 학습률 높게\n",
        "  * 자연어 처리에서 장점\n",
        "7. \n",
        "  * Adagrad + 모멘텀 -> Adam (가장 많이 사용됨)"
      ],
      "metadata": {
        "id": "uKHjrHWq1cGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#딥러닝 과대적합\n",
        "1. 드롭아웃\n",
        "2. 조기 종료"
      ],
      "metadata": {
        "id": "Qk8IC6uZ1_EV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 드롭아웃\n",
        "정의 :: 매개변수 일정량을 학습에 사용하지 않음 -> 모델의 분산을 효과적으로 줄임 -> 과대접함 위험 감소 (즉, 소규모 노드들이 다양하게 여러번 학습)"
      ],
      "metadata": {
        "id": "PwEdv92U7Zhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 조기 종료\n",
        "정의 :: 최대 학습 반복 횟수 설정, 과대적합 소지있을경우 학습 중단\n",
        "* 데이터 :: 학습 데이터, 검증 데이터, 테스트 데이터로 분리 (6:2:2가 적당)\n",
        "* 과정 :: 학습 데이터로만 도델 매개변수 조정 -> 검증 데이터로 모델 정확도 측정 -> 검증 데이터가 꾸준히 떨어지는 시점에 학습 중단 -> 최고의 모델로 테스트 진행"
      ],
      "metadata": {
        "id": "XM78qWJR74rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 퍼센트론[실습]\n",
        "뉴런 계산과정 & 활성화 함수 이해 최적  \n",
        "퍼셉트론은 스텝함수만 사용 ( 가중치*입력값+편향값 )  \n",
        "구조 :: 입력값 p1,p2,p3 / 가중치 w1,w2,w3 / 편향값 b  \n",
        "(T = 1.0, F = 0.0, bias = 1.0)가정"
      ],
      "metadata": {
        "id": "AnHceHB39WbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서프로우 임포트\n",
        "import tensorflow as tf\n",
        "T = 1.0\n",
        "F = 0.0\n",
        "bias = 1.0"
      ],
      "metadata": {
        "id": "m1aeq5MP-I2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 획득 (AND,OR,XOR)\n",
        "def get_AND_data():\n",
        "    X = [\n",
        "    [F, F, bias],\n",
        "    [F, T, bias],\n",
        "    [T, F, bias],\n",
        "    [T, T, bias]\n",
        "    ]\n",
        "    \n",
        "    y = [\n",
        "        [F],\n",
        "        [F],\n",
        "        [F],\n",
        "        [T]\n",
        "    ]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def get_OR_data():\n",
        "    X = [\n",
        "    [F, F, bias],\n",
        "    [F, T, bias],\n",
        "    [T, F, bias],\n",
        "    [T, T, bias]\n",
        "    ]\n",
        "    \n",
        "    y = [\n",
        "        [F],\n",
        "        [T],\n",
        "        [T],\n",
        "        [T]\n",
        "    ]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def get_XOR_data():\n",
        "    X = [\n",
        "    [F, F, bias],\n",
        "    [F, T, bias],\n",
        "    [T, F, bias],\n",
        "    [T, T, bias]\n",
        "    ]\n",
        "    \n",
        "    y = [\n",
        "        [F],\n",
        "        [T],\n",
        "        [T],\n",
        "        [F]\n",
        "    ]\n",
        "    \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "CLVIzrp7-OZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AND 데이터 호출\n",
        "X, y = get_AND_data()\n",
        "#X, y = get_OR_data()\n",
        "#X, y = get_XOR_data()"
      ],
      "metadata": {
        "id": "8TGcwcsK-yoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 생성 (perceptron 클래스)\n",
        "class Perceptron:\n",
        "    def __init__(self):\n",
        "        # 논리연산을 위한 입력값 X, Y와 편향값 (b)를 받을 것이므로, weight를 [3,1]로 설정합니다.  \n",
        "        # 3은 세개의 입력을 의미하고, 1은 한개의 뉴론임을 의미합니다.\n",
        "        self.W = tf.Variable(tf.random.normal([3, 1]))\n",
        "    \n",
        "    def train(self,X):\n",
        "        err = 1\n",
        "        epoch, max_epochs = 0, 20\n",
        "        while err > 0.0 and epoch < max_epochs:\n",
        "            epoch += 1\n",
        "            self.optimize(X)\n",
        "            # MSE (평균제곱오차)를 관찰하며, 학습이 진행되는 동안, 에러(MSE)가 줄어듬을 확인합니다.\n",
        "            err = self.mse(y, self.pred(X)).numpy()\n",
        "            print('epoch:', epoch, 'mse:', err)\n",
        "    \n",
        "    @tf.function\n",
        "    def faster_pred(self, X):\n",
        "        return self.step(tf.matmul(X, self.W))\n",
        "    \n",
        "    def pred(self, X):\n",
        "        return self.step(tf.matmul(X, self.W))\n",
        "       \n",
        "    def mse(self, y, y_hat):\n",
        "        return tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))\n",
        "    \n",
        "    def step(self,x):\n",
        "        # step(x) = { 1 if x > 0; 0 otherwise }\n",
        "        return tf.dtypes.cast(tf.math.greater(x, 0), tf.float32)\n",
        "\n",
        "    def optimize(self, X):\n",
        "        \"\"\"\n",
        "        퍼셉트론은 경사하강법을 사용한 최적화가 불가능합니다.\n",
        "        매번 학습을 진행할 때마다 가중치를 아래의 룰에 맞게 업데이트합니다.  \n",
        "\n",
        "        if target == 1 and activation == 0:  \n",
        "          w_new = w_old + input  \n",
        "\n",
        "        if target == 0 and activation == 1:  \n",
        "          w_new = w_old - input  \n",
        "\n",
        "        위의 두가지 조건은 아래의 코드로 간단히 구현 가능합니다.  \n",
        "        \"\"\"\n",
        "        delta = tf.matmul(X, tf.subtract(y, self.step(tf.matmul(X, self.W))), transpose_a=True)\n",
        "        self.W.assign(self.W+delta)"
      ],
      "metadata": {
        "id": "5Icuid6n-6Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 실행\n",
        "perceptron = Perceptron()\n",
        "perceptron.train(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dWkiGNh_Cs1",
        "outputId": "1c3487be-e287-4ce0-b46f-f89ef8022df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 mse: 0.25\n",
            "epoch: 2 mse: 0.75\n",
            "epoch: 3 mse: 0.25\n",
            "epoch: 4 mse: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 결과 확인\n",
        "print(perceptron.pred(X).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Oz9JKKx_QhV",
        "outputId": "b8961c79-2c76-4b9a-dc34-7b66f8e10d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n"
          ]
        }
      ]
    }
  ]
}